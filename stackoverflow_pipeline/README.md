# **STACKOVERFLOW PIPELINE**
A data pipeline created for Rittman Analytics that will process stackoverflow data available via GCP [public datasets](https://cloud.google.com/blog/products/gcp/google-bigquery-public-datasets-now-include-stack-overflow-q-a).
Trial is found within this [repo](https://github.com/rittmananalytics/analytics_engineer_trial).

## Using dbt & Developer Ways of Working (WoW)
- Use of virtual environments is recommended, especially if you work on multiple client/internal projects requiring conflicting dbt or Python versions
- Have you got the right version of dbt? See the **dbt_project.yml** file in root to check version limitations
- Set up your local profiles.yml and add it to your .dbt/profiles.yml file, you may need to direct dbt to this new path:  
 ``` $ export DBT_PROFILES_DIR=path/to/directory``` & ```$ export DBT_PROJECT_DIR=path/to/directory```.
- If running dbt from the command line, you will need to navigate ```cd folder/filepath``` into the stackoverflow_pipeline folder. 
- Run dbt debug to check your project setup and authentication is correct
- profiles.yml has 2 targets, **dev** & **prod**, and by default is set to dev
- Run the command ```./run``` for ease; this script runs ```dbt compile```, ```dbt run``` & ```dbt docs``` with 1 simple command.


## Tagging and Labelling

We add tags and labels to the resources generated by dbt, to assist with data discovery as well as enforcing our [Data Protection Approach].

- **Labels** are added to all BigQuery **datasets** in line with Datatonic's approach to [Resource Level Labelling](https://www.notion.so/datatonic/Resource-Level-Labelling-3f0e86f36e5f49d592a4d06fca13ce6e#4ff1a737801f48edbbfdff481a41a95a). These are created in the post_elt_task process which executes at the end of each dbt run. Dataset labels are defined as variabled in the dbt_project.yml file, and overridden for specific datasets within the post-hook itself. Changes or additions to this logic should be handled in the **update_dataset_labels** macro.
- **Policy tags** are added to specific BigQuery columns which have been identified as possibly sensitive (see Risk Classification and Data Protection Approach documents). These are added to the table schemas in the relevant YAML files, where the specific tag and taxonomy IDs are supplied as environment variables.
**dbt tags** are also applied via the dbt_project.yml file to enable targeted execution of the various layers and domains. See Folder Structure (below).

## Folder structure

The folder structure used in this dbt project is aligned to general best practices with 3 layers, a staging (ephemeral) layer,  warehouse (table) layer and finally an analytics (table) layer 

The structure is separated into distinct layers of transformation:
- Staging
- Warehouse
- Analytics

Each of these is defined in the ***dbt_project.yml*** file, along with descriptions and tags to enable targeted execution of a specific domain, transformation layer, or combination of the two.
![dt project structure](assets/dbt_project_structure.png)

## Data structure

The data structure within the warehouse layer is generally agnostic, and for the analytics layer follows a snowflake dim/fact schema (with conformed dimensions).  ***Please note, not all tables have been modelled, in the interest of time, modelling has been limited to relevant tables***  

- Full ERD diagram can be found [here](https://lucid.app/lucidchart/bca77f31-3eac-44b2-a93f-29b5fb831bab/edit?viewport_loc=66%2C375%2C2607%2C1422%2C0_0&invitationId=inv_73b3d199-5478-493a-9019-4c9a049c13f4)
- summarised version below:

![Summarised ERD Diagram](assets/StackoverflowStarSchema.png)

## SQLFluff

SQLFluff is used for linting and checking of code quality.  It is recommended to install and run SQLFluff locally prior to commiting your code to accelerate the development process. The rules enforced by SQLFluff can be found in the .sqlfluff file.

**Note:** when running SQLFluff from the command line, you must run it from the root. This ensures that the rules applied in the .sqlfluff file are selected rather than the default rules.  


## Other notes

- post_answers parent_id = id of post_questions
- one accepted answer per question (access via accepted_answer_id in post_questions [field is null in post_answers])
- vote table not very useful.....
- get topics from tags
- popular questions = highest view_count
- trending questions = most recent (creation_date)
- https://chatgpt.com/share/6738a471-8244-8001-8f8c-c943309960d3/continue


## Questions

- [ ] How many questions remain unanswered over time?
- [ ] What percentage of questions are unanswered? (by topic + total)
- [ ] Which tags/topics have the most unanswered questions?
- [ ] What types of questions are most likely to remain unanswered? (look at questions themselves - length, grammer, poorly written?) compare vs answered questions
- [ ] How quickly do questions with no accepted answer receive one?
- [ ] What are the highest-scored unanswered questions?
- [ ] What is the relationship between unanswered questions and new technologies?