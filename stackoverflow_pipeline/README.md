# **STACKOVERFLOW PIPELINE**
A data pipeline created for Rittman Analytics that will process stackoverflow data available via GCP [public datasets](https://cloud.google.com/blog/products/gcp/google-bigquery-public-datasets-now-include-stack-overflow-q-a).
Trial is found within this [repo](https://github.com/rittmananalytics/analytics_engineer_trial).

## Project Resources
**IMPORTANT** Please familiarise yourself with the project set up and design approaches:
- [High Level Solution Design](https://www.notion.so/datatonic/High-Level-Architecture-413747d5415d4d1fb3190a098b0f90ba)
- [Data Storage and Transformation Approach](https://www.notion.so/datatonic/Data-Storage-Transformation-Principles-11f17a9518d142669949c15dd113cb4b)
- [Data Protection Approach](https://www.notion.so/datatonic/IDP-Data-Protection-Approach-b245fb389ef14498acb313136eab9a93)

## Using dbt & Developer Ways of Working (WoW)
- Use of virtual environments is recommended, especially if you work on multiple client/internal projects requiring conflicting dbt or Python versions
- Have you got the right version of dbt? See the **dbt_project.yml** file in root to check version limitations
- Set up your local profiles.yml and add it to your .dbt/profiles.yml file, you will then need to direct dbt to this new path:  
 ``` $ export DBT_PROFILES_DIR=path/to/directory``` & ```$ export DBT_PROJECT_DIR=path/to/directory```.
- If running dbt from the command line, you will need to navigate [cd] into the stackoverflow_pipeline folder. 
- Run dbt debug to check your project setup and authentication is correct
- Read the Developer **Ways of Working** (below)
- Check that your target is set to **dev**
- Run the command ```./run``` , this script runs dbt compile, dbt run & dbt docs with 1 simple command.


## Tagging and Labelling

We add tags and labels to the resources generated by dbt, to assist with data discovery as well as enforcing our [Data Protection Approach].

- **Labels** are added to all BigQuery **datasets** in line with Datatonic's approach to [Resource Level Labelling](https://www.notion.so/datatonic/Resource-Level-Labelling-3f0e86f36e5f49d592a4d06fca13ce6e#4ff1a737801f48edbbfdff481a41a95a). These are created in the post_elt_task process which executes at the end of each dbt run. Dataset labels are defined as variabled in the dbt_project.yml file, and overridden for specific datasets within the post-hook itself. Changes or additions to this logic should be handled in the **update_dataset_labels** macro.
- **Policy tags** are added to specific BigQuery columns which have been identified as possibly sensitive (see Risk Classification and Data Protection Approach documents). These are added to the table schemas in the relevant YAML files, where the specific tag and taxonomy IDs are supplied as environment variables.
**dbt tags** are also applied via the dbt_project.yml file to enable targeted execution of the various layers and domains. See Folder Structure (below).

## Folder structure

The folder structure used in this dbt project is aligned to general best practices with 3 layers, a staging (ephemeral) layer,  
warehouse (table) layer and finally an analytics (table) layer 

The structure is separated into distinct layers of transformation:
- Staging
- Warehouse
- Analytics

Each of these is defined in the ***dbt_project.yml*** file, along with descriptions and tags to enable targeted execution of a specific domain, transformation layer, or combination of the two. 

## SQLFluff

SQLFluff is used for linting and checking of code quality. It is built into the CI workflow which runs against every PR in our GitHub repository, and the PR cannot be merged until all checks successfully pass. It is also recommended to install and run SQLFluff locally prior to commiting your code to accelerate the development process. The rules enforced by SQLFluff can be found in the .sqlfluff file.

**Note:** when running SQLFluff from the command line, you must run it from the root. This ensures that the rules applied in the .sqlfluff file are selected rather than the default rules.  


## OTHER THINGS TO COMPLETE

- post_answers parent_id = id of post_questions
- one accepted answer per question (access via accepted_answer_id in post_questions [field is null in post_answers])
- vote table not very useful.....
- get topics from tags
- popular questions = highest view_count
- trending questions = most recent (creation_date)
- https://chatgpt.com/share/6738a471-8244-8001-8f8c-c943309960d3/continue

